import os

import pandas as pd

# ==== File Paths ====
source_file = r"C:\Users\Sourav.Sindhikar\Downloads\Mapping\Alliance\Alliance_Parent_V1.csv"

vlookup_file = r"C:\Users\Sourav.Sindhikar\Downloads\Mapping\Vlookup_mapping_file 5 2.csv"

output_file = r"C:\Users\Sourav.Sindhikar\Downloads\Mapping\Output 2 - Usermapping\With Parent\UserMapping-Alliance_Parent_V1.csv"

# ==== Remove previous output file if exists ====
if os.path.exists(output_file):
    os.remove(output_file)
    print(f"‚ö†Ô∏è Existing output file removed: {output_file}")

# ==== Load Vlookup File ====
vlookup_df = pd.read_csv(vlookup_file, dtype=str).fillna("")

# ==== Clean Whitespace ====
for col in vlookup_df.columns:
    vlookup_df[col] = vlookup_df[col].astype(str).str.strip()

# ==== Build Lookup Dictionaries (Case-Insensitive) ====
dict_b_to_c = {k.lower(): v for k, v in zip(vlookup_df["digital_prod_Id"], vlookup_df["digital_Global_ID__c"])}

dict_g_to_f = {k.lower(): v for k, v in zip(vlookup_df["merge_Global_ID__c"], vlookup_df["merge_Id"])}

dict_email_name_to_f = {
    (str(row["merge_email"]).lower(), str(row["merge_Name"]).lower()): row["merge_Id"]
    for _, row in vlookup_df.iterrows()
}

dict_b_to_email_name = {
    str(row["digital_prod_Id"]).lower(): (str(row["digital_Email"]).lower(), str(row["digital_Name"]).lower())
    for _, row in vlookup_df.iterrows()
}

# ==== Mapping Function ====
def map_id(digital_prod_id):
    if not digital_prod_id:
        return ""

    digital_prod_id_lc = str(digital_prod_id).strip().lower()

    # Check global ID mapping
    digital_global_id = dict_b_to_c.get(digital_prod_id_lc, "").lower()
    if digital_global_id and digital_global_id in dict_g_to_f:
        return dict_g_to_f[digital_global_id]

    # Check email + name mapping
    email_name = dict_b_to_email_name.get(digital_prod_id_lc)
    if email_name:
        email_name_lc = (email_name[0].lower(), email_name[1].lower())
        return dict_email_name_to_f.get(email_name_lc, "")

    return ""

# ==== Default Values ====
DEFAULT_OWNER_ID = "005Wr00000ECxAjIAL"
DEFAULT_CREATEDBY_LASTMODIFIED_ID = "005A0000000rXeVIAU"

# ==== Process Source File in Chunks ====
chunk_size = 50000
reader = pd.read_csv(source_file, dtype=str, chunksize=chunk_size)
header_written = False
total_rows = 0

for i, chunk in enumerate(reader, start=1):
    chunk = chunk.fillna("")

    # Columns to map
    columns_to_map = ["OwnerId", "CreatedById", "LastModifiedById"]

    # Replace original columns with mapped values and fill defaults
    for col in columns_to_map:
        if col in chunk.columns:
            # Store original values to check if source has value
            original_values = chunk[col].copy()
            
            # Replace original column with mapped values
            chunk[col] = chunk[col].map(map_id)
            
            # Fill default values: only if source has value but mapping is empty
            if col == "OwnerId":
                # For OwnerId: fill with default if source has value but mapping is empty
                mask = (original_values.astype(str).str.strip() != "") & (chunk[col].astype(str).str.strip() == "")
                chunk.loc[mask, col] = DEFAULT_OWNER_ID
            elif col in ["CreatedById", "LastModifiedById"]:
                # For CreatedById and LastModifiedById: fill with default if source has value but mapping is empty
                mask = (original_values.astype(str).str.strip() != "") & (chunk[col].astype(str).str.strip() == "")
                chunk.loc[mask, col] = DEFAULT_CREATEDBY_LASTMODIFIED_ID
        else:
            print(f"‚ö†Ô∏è Column '{col}' not found in chunk {i}, skipping...")

    # Write to output file
    if not header_written:
        # First chunk: write mode with header
        chunk.to_csv(output_file, index=False, mode="w", header=True, encoding="utf-8-sig")
        header_written = True
    else:
        # Subsequent chunks: append mode without header
        chunk.to_csv(output_file, index=False, mode="a", header=False, encoding="utf-8-sig")

    total_rows += len(chunk)
    print(f"‚úÖ Processed chunk {i} ({len(chunk)} rows)")

print(f"‚úÖ Mapping completed successfully. Output saved to: {output_file}")
print(f"üìä Total rows processed: {total_rows}")
